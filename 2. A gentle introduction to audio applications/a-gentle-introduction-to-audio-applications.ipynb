{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I. Audio classification with a pipeline\n\nAudio classification involves **assigning one or more labels to an audio recording** based on its content.\nThe labels could correspond to different sound categories, such as music, speech, or noise, or more specific categories like bird song or car engine sounds.  \n\n**Example:** the MINDS-14 dataset that contains recordings of people asking an e-banking system questions in several languages and dialects, and has the intent_class for each recording.\n\n","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom datasets import Audio\n\nminds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\nminds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-01T21:58:02.876844Z","iopub.execute_input":"2024-11-01T21:58:02.877241Z","iopub.status.idle":"2024-11-01T21:59:47.814890Z","shell.execute_reply.started":"2024-11-01T21:58:02.877193Z","shell.execute_reply":"2024-11-01T21:59:47.813683Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"minds14.py:   0%|          | 0.00/5.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0708fb6b48d441a38e06e2d37174d128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.28k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"302b1f6f91364c4d9c4fddd756b893d6"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for PolyAI/minds14 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/PolyAI/minds14.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"MInDS-14.zip:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b689a8e6ce94fa1ad398593e286b967"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"281081e1bdb445df83ff53596f82fdb2"}},"metadata":{}}]},{"cell_type":"markdown","source":"# II. Automatic Speech Recognition (ASR) with a pipeline\n\nASR is a task that involves transcribing speech audio recording into text.\n\nIn this section, weâ€™ll use the automatic-speech-recognition pipeline to transcribe an audio recording of a person asking a question about paying a bill using the same MINDS-14 dataset as before.","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\n# instantiate the pipeline\nasr = pipeline(\"automatic-speech-recognition\")\n\n#  take an example from the dataset and pass its raw data to the pipeline\nexample = minds[0]\nasr(example[\"audio\"][\"array\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T22:00:31.859228Z","iopub.execute_input":"2024-11-01T22:00:31.859731Z","iopub.status.idle":"2024-11-01T22:01:14.557378Z","shell.execute_reply.started":"2024-11-01T22:00:31.859685Z","shell.execute_reply":"2024-11-01T22:01:14.555558Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c05c001b085745989c34cf3a0767afe8"}},"metadata":{}},{"name":"stderr","text":"No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 22aad52 (https://huggingface.co/facebook/wav2vec2-base-960h).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a84573f0681d4d45867f0fbb58c05291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb7e17de1bc245fba2e11fe8455d564e"}},"metadata":{}},{"name":"stderr","text":"Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae1db5d6b794223a5840a4e39d9b4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb957b0b1c8a498d8fb74667887010bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddabca799db040d1851ba14fc3e835f1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afd9b0f96e3043599f295c26c409470e"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"{'text': 'I WOULD LIKE TO PAY MY ELECTRICITY BILL USING MY CAD CAN YOU PLEASE ASSIST'}"},"metadata":{}}]},{"cell_type":"markdown","source":"# III. Audio generation with a pipeline\n\nAudio generation encompasses a versatile set of tasks that involve producing an audio output. \nThe tasks that we will look into here are speech generation (aka â€œtext-to-speechâ€) and music generation. \n\n- In text-to-speech, a model transforms a piece of text into lifelike spoken language sound, opening the door to applications such as virtual assistants, accessibility tools for the visually impaired, and personalized audiobooks.\n- On the other hand, music generation can enable creative expression, and finds its use mostly in entertainment and game development industries.\n\n\nIn ðŸ¤— Transformers, youâ€™ll find a pipeline that covers both of these tasks. \nThis pipeline is called \"text-to-audio\", but for convenience, it also has a \"text-to-speech\" alias. \n","metadata":{}},{"cell_type":"markdown","source":"### 1. Generating speech","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"text-to-speech\", model=\"suno/bark-small\")\n\n# passing some text through the pipeline; all the preprocessing will be done for us under the hood:\ntext = \"Ladybugs have had important roles in culture and religion, being associated with luck, love, fertility and prophecy. \"\noutput = pipe(text)\n\n# In a notebook, we can use the following code snippet to listen to the result:\nfrom IPython.display import Audio\nAudio(output[\"audio\"], rate=output[\"sampling_rate\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T22:04:38.185718Z","iopub.execute_input":"2024-11-01T22:04:38.187779Z","iopub.status.idle":"2024-11-01T22:04:39.364030Z","shell.execute_reply.started":"2024-11-01T22:04:38.187697Z","shell.execute_reply":"2024-11-01T22:04:39.362190Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# passing some text through the pipeline; all the preprocessing will be done for us under the hood:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLadybugs have had important roles in culture and religion, being associated with luck, love, fertility and prophecy. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_to_audio.py:186\u001b[0m, in \u001b[0;36mTextToAudioPipeline.__call__\u001b[0;34m(self, text_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params):\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;03m    Generates speech/audio from the inputs. See the [`TextToAudioPipeline`] documentation for more information.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m        - **sampling_rate** (`int`) -- The sampling rate of the generated audio waveform.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1268\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1261\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1274\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1275\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_to_audio.py:147\u001b[0m, in \u001b[0;36mTextToAudioPipeline._forward\u001b[0;34m(self, model_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# generate_kwargs get priority over forward_params\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     forward_params\u001b[38;5;241m.\u001b[39mupdate(generate_kwargs)\n\u001b[0;32m--> 147\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(generate_kwargs):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bark/modeling_bark.py:1724\u001b[0m, in \u001b[0;36mBarkModel.generate\u001b[0;34m(self, input_ids, history_prompt, return_output_lengths, **kwargs)\u001b[0m\n\u001b[1;32m   1721\u001b[0m             kwargs_fine[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;66;03m# 1. Generate from the semantic model\u001b[39;00m\n\u001b[0;32m-> 1724\u001b[0m semantic_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msemantic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43msemantic_generation_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msemantic_generation_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs_semantic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;66;03m# 2. Generate from the coarse model\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m coarse_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoarse_acoustics\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1733\u001b[0m     semantic_output,\n\u001b[1;32m   1734\u001b[0m     history_prompt\u001b[38;5;241m=\u001b[39mhistory_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_coarse,\n\u001b[1;32m   1740\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bark/modeling_bark.py:900\u001b[0m, in \u001b[0;36mBarkSemanticModel.generate\u001b[0;34m(self, input_ids, semantic_generation_config, history_prompt, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    894\u001b[0m early_stopping_logits_processor \u001b[38;5;241m=\u001b[39m BarkEosPrioritizerLogitsProcessor(\n\u001b[1;32m    895\u001b[0m     eos_token_id\u001b[38;5;241m=\u001b[39msemantic_generation_config\u001b[38;5;241m.\u001b[39meos_token_id, min_eos_p\u001b[38;5;241m=\u001b[39mmin_eos_p, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    896\u001b[0m )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;66;03m# pass input_ids in order to stay consistent with the transformers generate method even though it is not used\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;66;03m# (except to get the input seq_len - that's why we keep the first 257 tokens)\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m semantic_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    901\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones((batch_size, max_input_semantic_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    902\u001b[0m     input_embeds\u001b[38;5;241m=\u001b[39minput_embeds,\n\u001b[1;32m    903\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39m[suppress_tokens_logits_processor, early_stopping_logits_processor],\n\u001b[1;32m    904\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39msemantic_generation_config,\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    906\u001b[0m )  \u001b[38;5;66;03m# size: 10048\u001b[39;00m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;66;03m# take the generated semantic tokens\u001b[39;00m\n\u001b[1;32m    909\u001b[0m semantic_output \u001b[38;5;241m=\u001b[39m semantic_output[:, max_input_semantic_length \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]\n","\u001b[0;31mTypeError\u001b[0m: transformers.generation.utils.GenerationMixin.generate() got multiple values for keyword argument 'generation_config'"],"ename":"TypeError","evalue":"transformers.generation.utils.GenerationMixin.generate() got multiple values for keyword argument 'generation_config'","output_type":"error"}]},{"cell_type":"code","source":"song = \"â™ª In the jungle, the mighty jungle, the ladybug was seen. â™ª \"\noutput = pipe(song)\nAudio(output[\"audio\"], rate=output[\"sampling_rate\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Generating music","metadata":{}},{"cell_type":"code","source":"music_pipe = pipeline(\"text-to-audio\", model=\"facebook/musicgen-small\")\n\n# create a text description of the music weâ€™d like to generate\ntext = \"90s rock song with electric guitar and heavy drums\"\n\n# control the length of the generated output by passing an additional max_new_tokens parameter to the model\nforward_params = {\"max_new_tokens\": 512}\n\noutput = music_pipe(text, forward_params=forward_params)\nAudio(output[\"audio\"][0], rate=output[\"sampling_rate\"])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T22:05:57.216626Z","iopub.execute_input":"2024-11-01T22:05:57.217660Z","iopub.status.idle":"2024-11-01T22:08:50.831801Z","shell.execute_reply.started":"2024-11-01T22:05:57.217610Z","shell.execute_reply":"2024-11-01T22:08:50.830067Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/7.87k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7bdeeb72ac24c8d949486afe604cc63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.36G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fb627e2225642edb23a4db42707650d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/224 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b5dc23c2a1b45be9d40508bd80e9c3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.37k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5592a134e6d145fca7c924d6d0227573"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a85e571e1b0441f5ae48fc337e6d13c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e32fe093434748f192d9ee57768e75b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b119e0b0c26443586a499ec8e1c19d4"}},"metadata":{}},{"name":"stderr","text":"`torch.nn.functional.scaled_dot_product_attention` does not support having an empty attention mask. Falling back to the manual attention implementation. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.Note that this probably happens because `guidance_scale>1` or because you used `get_unconditional_inputs`. See https://github.com/huggingface/transformers/issues/31189 for more information.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m forward_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_new_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m512\u001b[39m}\n\u001b[1;32m      9\u001b[0m output \u001b[38;5;241m=\u001b[39m music_pipe(text, forward_params\u001b[38;5;241m=\u001b[39mforward_params)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mAudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: Audio.__init__() got an unexpected keyword argument 'rate'"],"ename":"TypeError","evalue":"Audio.__init__() got an unexpected keyword argument 'rate'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
